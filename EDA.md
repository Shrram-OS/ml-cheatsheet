# EDA (Exploratory Data Analysis) -  разведовательный анализ данных (полная хуйня, а не название, как по мне)

## 1. Че за хуйня
Смотрим что у нас вообще в данных. (По сути не важно, если не дегенерат).

Однако, стоит реально глянуть чё в данных (см. Графики) и посмотреть на странные вещи. Понять насколько "корректный" датасет. Пример: Странно будет если у вас в категориальном признаке "есть или нету лифта в доме?" (0 или 1) будет число 666, верно? 

Всякие `.head()`; `.info()`

## 2. Чистим

### Пропуски
`.isna().sum()` - Покажет есть ли пропуски.
Машинное обучение дорогое и во многих функциях под капотом практически все методы для оптимизации проверяют размерность и ты в любом случае получишь ошибку если есть пропуски.

`.dropna()` - удалит все пропуски. Только когда их мало или заменить очень сложно.

`.fillna(value, method, axis, inplace, limit)` - метод для замены пропусков.

есть модели, которым похуй (CatBoost, LightGBM), но это уже нюансы

### Исправление типов 
Спаршенные уебанские даты в нормальный datetime64[ns].

Числа спарсились в longdouble, но там только int-ы -> пожалейте компьютер.

`"Нью-Йорк"` != `"нью-йорк"`. (опять же если вы не дегенерат)

### Дубликаты - `.drop_duplicates()`
Есть "скрытые" дубликаты, пример: разные айди, одинаковый остальные строки, стоит учесть.

## 3. Графики. 
Их обычно делят на одномерные и не одномерные, причём абсолютно не понятно нахуя.

#### Одномерные
`.hist()` - makes sense по сути по всем графикам. Посмотреть есть ли выбросы и глянуть поверхностно нормальные ли вообще данные. (Существуют всякие приколы типа закона о больших числах и различные распределения которые встречаются в природе, которые должны соответствовать. Зарплата - логнормальное распределение. Рост/Вес - нормальное (гауссовское) распределение, перерывы между звонками в колл-центр - экспоненциальное распределение)

`.boxplot()` - тоже для выбросов полезно. Для медиан, квантилей, распределения.

#### Не одномерные
`.scatter()` это та самая с которой построишь счастливую жизнь, а `.regplot()` это сексуальная молоденькая которая готова прямо сейчас. 

``` python
.heatmap(
    df.corr(),
    annot=True,
    ...
)
```
Ебануто мощная хуйня. Смотришь зависимость признаков. Находишь мультиколлинеарности. Пытаешься избежать утечку данных.

## 4. Feature Engineering
фьючер инжиниринг - наука о блаблабла

#### Разберём на примере linear / logistic regression:
(см. [Linear Regression](linear%20regression.md))

Есть два варианта (рекомендую второй)

### Абстрактно / Магия
Feature Engineering - это процесс преобразования необработанных данных в соответствующую информацию для использования в моделях машинного обучения. (я тоже ничего не понял).

Это использование уже существующих данных (признаков) для создания новых, помогающих модели лучше "понять" природу целового признака. 

Примеры (всех заебавшая задача с предугадыванием цен на квартиры):
1. `total_area / rooms` - "Какая средняя площадь комнаты?"
2. `is_capital` (boolean) - "Находится ли квартира в столице?"
3. `age = current_year - year_built` - "Сколько лет дому?"

Однако в таком случае рекомендую пострить regplot и/или глянуть таблицу корреляций, потому-что вы можете создать признак который будет только добавлять шум и ухудшит модель.

### Под капотом
Каждая модель под капотом - точно не магия и имеет какой-то алгоритм и/или формулу. 

Feature Engineering - это математическое проектирование формы функции.

Примеры (каждому прочитать до 3, рекомендую до 5, фрики до 8):
1. Смещение.
- Добавляем категориальный признак 
- Модель рисует две (или более) параллельные прямые на разной высоте.
- $y = \beta_0+\beta_1x+\beta_2[flag]$
2. Изгиб
- Возводим признак в степень ($x^2, x^3, \sqrt{(x)}$).
- Прямая превращается в параболу, гиперболу или сложную волну.
- $y = \beta_0+\beta_1x+\beta_2x^2$
3. Перелом 
- Добавляем Interaction Term (произведение признаков).
- В одной группе прямая может идти горизонтально, в другой - резко вверх. Прямые перестают быть параллельными
- $y = \beta_0+\beta_1x+\beta_2(x*flag)$
4. Отражение (Симметрия) 
- Применяем модуль $|x|$ или центрирование $(x-mean)^2$
- Модель может описать V - образные зависимости (например цена ошибки: плохо и когда слишком мало и когда слишком много).
- $y = \beta_0+\beta_1|x|$
5. Ступеньки (Дискретизация)
- Разрезаем непрерывное число на интервалы (Binning) и провращаем их в набор (One-Hot).
- Модель рисует "лестницу" - фиксированные значение на определенных отрезках.
- $y = \beta_0+\beta_1$[0_to_10]+$\beta_2$[10_to_20]
6. Периодичность
- Применяем тригонометрию ($sin(x), cos(x)$)
- Модель учится предсказывать сезонность (например, всплески продаж каждое лето).
- $y = \beta_0+\beta_1sin(x)$
7. Сжатие и растяжение
- Берём $\log(x)$ или $\mathrm{e}^x$
- Модель начинает понимать "проценты" вмсто "штук". Когда каждое следующее увеличение $x$ дает всё меньший эффект (закон убывающей отдачи).
- $y = \beta_0+\beta_1\log(x)$
8. Точки разрыва
- Комбинируем флаг и значение
- Прямая идет-идет, потом внезапно обрывается и продолжается в другом месте (эффект "порога" или "штрафа")






### Что надо знать каждому 
Feature Engineering — это управление формой функции. Чаще всего ты делаешь одно из следующих:

Сдвигаешь зависимость (bias, флаги) — когда у разных групп одинаковый наклон, но разный уровень. Пример: Если кенты твоего бати бухают и поют "Дожди-пистолеты", то их голос будет пропорционален количеству бухла $y = алкашка\cdot w$, но если дядя с (назовём его Иван) ходил на курсы вокала, то у него базовая громкость выше на 40 децибел $y = алкашка\cdot w+{is\_Ivan}?\cdot40$.

Гнёшь зависимость (степени, корни) — когда связь не прямая. Пример: насколько сильно очередной хрустик на мотике вревратится в фарш после поворота? 5 км/ч - царапина, 50 км/ч - джекпот.

Ломаешь зависимость (interactions) — когда влияние признака зависит от другого. Пример: твой кент с ножом, одно дело если он пошёл нарезать закуску, а другое если он уже давно нарезал закуску. В контексте "Кентафарик пошёл на кухню готовить" - это продуктивность, а "Кентафарик нажрался и щяс всех перережёт" это ...... ну.... немного другое $y = нож \cdot кент$

Отражаешь зависимость (модуль, центрирование) — когда ошибка важна в обе стороны. Пример: <span style="opacity: 0.50;">Меня тоже заебали бухие аналогии, но тут невозможно удержаться.</span> Идеально - ебануть вискарик 40 $^{\circ}$. 10 $^{\circ}$ - для девушек. Но и при 70 $^{\circ}$ тот самый кентафарик (в пункте выше) всех реально перехуярит. Моделируем как $|T-30|$

Квантуешь зависимость (binning) — когда точность не важна, важен диапазон. Бинирование - не категориальные признаки -> в категориальные признаки. Пример: bmi > 25 -> жирный уебок (я кстати такой); bmi < 18 -> худой уебок.

Делаешь периодической (sin/cos) — когда есть сезонность. Пример: уровень политического срача в коментах тгк. В 12:00 - затишье, в 24:00 - всем надо высказать свое мнение. Если давать модели часы от 0 до 23, она не поймёт что 23:59 и 00:01 это почти одно и то же время. Натянем часы на циферблат с помощью косинусов и синусов

Сжимаешь / растягиваешь (log / exp) — когда модель должна понимать проценты, а не штуки. Логарифмирование - `np.log1p()` если `scipy.stats.skew()` $\approx$>  0.8. Нужно для данных с большим хвостом, чтобы модель была устойчивее к выбросам. Примеры: заплаты, доход, кол-во пользователей, количество ссылок на сайты. Пример: разница между 100 и 500 грывень - пропасть, а между 1 000 100 и 1 000 500 - статистическая погрешность.

Вводишь пороги (флаги + значения) — когда после некоторого значения мир меняется. Пример: послушать 1-3 грустных трека - терпимо, но после 4 уже вспоминаешь бывшую. Суть в том что после $X$ коэффициент при признаке может не просто измениться, а включить совершенно другой механизм поведения системы.

Изучите тему над которой работаете и используйте мозг. 


**ВАЖНО:** НИКОГДА не допускайте "утечки". Утечка данных (data leakage) - это использование информации для обучения модели, которой у модели не должно быть или не будет во время предсказания. Пример: `area/price`, ты якобы интуитивно думаешь "сколько стоит кв.м.", но ты просто даешь модели подсмотреть ответ наперёд.


### 5. Думайте головой
Всегда анализируйте в общей картине. Изучите просто ту тему над которой вы работаете. Обдумайте каждый признак и что он значит (если это возможно конечно). 

Например, есть некоторые данные содержащие выбросы, однако они могут быть ценными редкими данными которые имеет смысл просто разбинить.

Никогда не используйте целевой признак для создания новых


